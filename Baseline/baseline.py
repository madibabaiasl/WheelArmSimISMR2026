# -*- coding: utf-8 -*-
"""baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dlCpD1Jjb5yBb-qisJuo0H59dHTgJGkM
"""

import torch
import torch.nn as nn
from torchvision.models import resnet18

class MultiModalityModel(nn.Module):
    def __init__(self, num_labels, pred_dim, num_trials, word2vec_matrix, lstm_hidden_dim=128, lstm_layers=1):
        """
        Args:
            num_labels (int): Number of classes for classification.
            pred_dim (int): Dimension of the predicted next action.
            num_trials (int): Number of distinct trials (for trial embedding).
            word2vec_matrix (np.array): Pre-trained word2vec embeddings with shape (num_vocab, w2v_dim).
            lstm_hidden_dim (int): Hidden dimension for the LSTM.
            lstm_layers (int): Number of LSTM layers.
        """
        super().__init__()
        # RGB encoder (shared for both images)
        self.rgb = resnet18(pretrained=True)
        self.rgb.fc = nn.Identity()
        for param in self.rgb.parameters():
            param.requires_grad = False

        # Depth encoder (shared for both depth images)
        self.depth_cnn = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(32, 64)
        )

        # Other encoders
        self.state_encoder = nn.Sequential(
            nn.Linear(112, 64),
            nn.ReLU(),
            nn.LayerNorm(64)
        )
        # Initialize the instruction embedding layer with pre-trained word2vec embeddings.
        # word2vec_matrix should be a NumPy array of shape (num_vocab, w2v_dim)
        word2vec_tensor = torch.tensor(word2vec_matrix, dtype=torch.float32)
        self.instruction_embedding = nn.Embedding.from_pretrained(word2vec_tensor, freeze=False)
        # Use a linear layer to map the word2vec embedding dimension (e.g., 300) to 64.
        self.instruction_fc = nn.Linear(word2vec_tensor.size(1), 64)

        self.time_encoder = TimeEmbedding(embed_dim=16)
        self.trial_encoder = TrialEmbedding(num_trials, embed_dim=16)

        # Fusion module
        # total_feat_dim = (512 * 2) + (64 * 2) + 64 + 64 + 16 + 16 = 1312
        self.fusion = FusionModule(1312, output_dim=128)

        # Sequence modeling: LSTM to capture temporal dependencies
        self.lstm = nn.LSTM(
            input_size=128,
            hidden_size=lstm_hidden_dim,
            num_layers=lstm_layers,
            batch_first=True
        )

        # Heads for classification and next action prediction.
        self.classifier = nn.Linear(lstm_hidden_dim, num_labels)
        self.predictor = nn.Linear(lstm_hidden_dim, pred_dim)

    def forward(self, image1_seq, image2_seq, instruction_seq, action_seq,
                depth1_seq, depth2_seq, time_seq, trial_id_seq):
        """
        Expected shapes:
            image1_seq, image2_seq: (batch_size, seq_len, C, H, W)
            depth1_seq, depth2_seq: (batch_size, seq_len, H, W)  -- single channel images
            instruction_seq: (batch_size, seq_len, instr_len)  -- token indices for each instruction
            action_seq: (batch_size, seq_len, 112)              -- state info for actions
            time_seq: (batch_size, seq_len)                     -- scalar time values per timestep
            trial_id_seq: (batch_size, seq_len)                 -- trial IDs per timestep
        """
        batch_size, seq_len = image1_seq.size(0), image1_seq.size(1)
        fused_features = []

        # Process each time step independently
        for t in range(seq_len):
            # Extract time step t for each modality
            image1 = image1_seq[:, t]
            image2 = image2_seq[:, t]
            depth1 = depth1_seq[:, t]
            depth2 = depth2_seq[:, t]
            # instruction: (batch_size, instr_len) -- token indices
            instruction = instruction_seq[:, t]
            action = action_seq[:, t]
            time = time_seq[:, t]
            trial_id = trial_id_seq[:, t]

            # Process RGB images using the frozen ResNet18 encoder.
            image_feat1 = self.rgb(image1)
            image_feat2 = self.rgb(image2)
            image_feat = torch.cat([image_feat1, image_feat2], dim=1)

            # Process depth images.
            depth_feat1 = self.depth_cnn(depth1)
            depth_feat2 = self.depth_cnn(depth2)
            depth_feat = torch.cat([depth_feat1, depth_feat2], dim=1)

            # Process text instructions with pre-trained word2vec.
            # Look up embeddings: (batch_size, instr_len, w2v_dim)
            embedded_instr = self.instruction_embedding(instruction)
            # Average the token embeddings to get a fixed-length vector: (batch_size, w2v_dim)
            avg_instr = embedded_instr.mean(dim=1)
            # Map to a 64-dimensional feature vector.
            instr_feat = self.instruction_fc(avg_instr)

            # Process other features.
            action_feat = self.state_encoder(action)
            time_feat = self.time_encoder(time)
            trial_feat = self.trial_encoder(trial_id)

            # Fuse all features into one vector for the current time step.
            fused = self.fusion(image_feat, instr_feat, action_feat,
                                depth_feat, time_feat, trial_feat)
            fused_features.append(fused)

        # Stack the fused features to form a sequence: (batch_size, seq_len, 128)
        fused_seq = torch.stack(fused_features, dim=1)

        # Pass the sequence through the LSTM.
        lstm_out, _ = self.lstm(fused_seq)

        # For next timestep prediction, use the last LSTM output.
        next_action_pred = self.predictor(lstm_out[:, -1, :])

        # For classification, aggregate information over time (here using mean pooling).
        cls_logits = self.classifier(lstm_out.mean(dim=1))

        return cls_logits, next_action_pred

# FusionModule remains unchanged.
class FusionModule(nn.Module):
    def __init__(self, input_dim, output_dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, output_dim)
        )

    def forward(self, image_feat, instr_feat, action_feat, depth_feat, time_feat, trial_feat):
        concatenated = torch.cat([
            image_feat, instr_feat, action_feat,
            depth_feat, time_feat, trial_feat
        ], dim=1)
        return self.mlp(concatenated)

class TimeEmbedding(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.embed = nn.Linear(1, embed_dim)

    def forward(self, time):
        # time is expected as (batch_size,) -> unsqueeze to (batch_size, 1)
        return torch.sin(self.embed(time.unsqueeze(1)))

class TrialEmbedding(nn.Module):
    def __init__(self, num_trials, embed_dim):
        super().__init__()
        self.embed = nn.Embedding(num_trials, embed_dim)

    def forward(self, trial_id):
        return self.embed(trial_id)

from google.colab import drive
drive.mount('/content/drive')
n = 1
csv_path = f'/content/drive/MyDrive/data/organization/picking/pick_mustard/Experiment{n}/final_data.csv'

import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import torchvision.transforms as transforms
from PIL import Image
import os

class RobotTaskDataset(Dataset):
    def __init__(self, csv_task_path, num_trials, word2vec_model):
        """
        Args:
            csv_task_path (str): Path to the directory containing CSV files.
            num_trials (int): Number of experiments/trials.
            word2vec_model: A pre-trained word2vec model. This can be a Gensim KeyedVectors object
                            or a dictionary mapping tokens to embedding vectors.
        """
        self.csv_task_path = csv_task_path
        self.num_trials = num_trials
        self.word2vec = word2vec_model

        # Determine embedding dimension from the word2vec model.
        try:
            self.embedding_dim = self.word2vec.vector_size
        except AttributeError:
            # Assume word2vec_model is a dict; get dimension from the first vector.
            first_key = next(iter(self.word2vec))
            self.embedding_dim = len(self.word2vec[first_key])

        # Prepare lists to hold folder paths.
        self.gen3_rgb = []
        self.gen3_depth = []
        self.wheelchair_rgb = []
        self.wheelchair_depth = []
        data_frames = []

        for n in range(1, num_trials + 1):
            csv_path = os.path.join(self.csv_task_path, f'Experiment{n}', 'final_data.csv')
            # Build folder paths for the different modalities.
            self.wheelchair_rgb.append(csv_path.replace('final_data.csv', 'wheelchair_rgb/'))
            self.wheelchair_depth.append(csv_path.replace('final_data.csv', 'wheelchair_depth/'))
            self.gen3_rgb.append(csv_path.replace('final_data.csv', 'gen3_rgb/'))
            self.gen3_depth.append(csv_path.replace('final_data.csv', 'gen3_depth/'))

            df = pd.read_csv(csv_path)
            # Compute a frame number for each row per trial (if needed).
            df['frame_number'] = df.groupby('trial_id').cumcount()
            data_frames.append(df)

        # Concatenate all CSV data into one DataFrame.
        self.data = pd.concat(data_frames, ignore_index=True)

    def tokenize_instructions(self, instruction):
        """
        Tokenizes the instruction string using whitespace, looks up the pre-trained word2vec
        embedding for each token, and returns the average embedding as a torch tensor.
        """
        tokens = instruction.split()
        vectors = []
        for token in tokens:
            # Check if the token exists in the word2vec model.
            # For a Gensim model you can do: "if token in self.word2vec"
            # (it supports key membership testing).
            if token in self.word2vec:
                vec = self.word2vec[token]
            else:
                # Use the '<unk>' embedding if available; otherwise, use a zero vector.
                if '<unk>' in self.word2vec:
                    vec = self.word2vec['<unk>']
                else:
                    vec = [0.0] * self.embedding_dim
            vectors.append(vec)

        # If no tokens were found, return a zero vector.
        if len(vectors) == 0:
            avg_vec = [0.0] * self.embedding_dim
        else:
            # Average over the token vectors.
            avg_vec = [sum(x) / len(vectors) for x in zip(*vectors)]
        return torch.tensor(avg_vec, dtype=torch.float32)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]

        # Process the instruction using word2vec.
        instruction_str = row['instructions']
        instruction_embedding = self.tokenize_instructions(instruction_str)
        # instruction_embedding will be a tensor of shape (embedding_dim,)

        # Process other data (action, time, trial, label).
        action_values = [row.iloc[i] for i in range(0, 112)]
        action = torch.tensor(action_values, dtype=torch.float32)
        time_info = torch.tensor([float(row['sim_time'])], dtype=torch.float32)
        trial_id_val = int(row['trial_id'])
        trial_id = torch.tensor(trial_id_val, dtype=torch.long)
        label = torch.tensor(int(row['state_labels']), dtype=torch.long)

        # Get the precomputed frame number for constructing image paths.
        frame_number = int(row['frame_number'])
        gen3_rgb_path = os.path.join(self.gen3_rgb[trial_id_val], f"frame_{frame_number}.png")
        gen3_depth_path = os.path.join(self.gen3_depth[trial_id_val], f"frame_{frame_number}.png")
        wheelchair_rgb_path = os.path.join(self.wheelchair_rgb[trial_id_val], f"frame_{frame_number}.png")
        wheelchair_depth_path = os.path.join(self.wheelchair_depth[trial_id_val], f"frame_{frame_number}.png")

        # Load images (with fallbacks if files are missing).
        if os.path.exists(gen3_rgb_path):
            gen3_rgb_image = Image.open(gen3_rgb_path).convert("RGB")
        else:
            print(f"Warning: {gen3_rgb_path} not found. Using a blank image.")
            gen3_rgb_image = Image.new("RGB", (224, 224))

        if os.path.exists(gen3_depth_path):
            gen3_depth_image = Image.open(gen3_depth_path).convert("L")
            gen3_depth_image = transforms.ToTensor()(gen3_depth_image)
        else:
            print(f"Warning: {gen3_depth_path} not found. Using a blank depth image.")
            gen3_depth_image = torch.zeros((1, 64, 64))

        if os.path.exists(wheelchair_rgb_path):
            wheelchair_rgb_image = Image.open(wheelchair_rgb_path).convert("RGB")
        else:
            print(f"Warning: {wheelchair_rgb_path} not found. Using a blank image.")
            wheelchair_rgb_image = Image.new("RGB", (224, 224))

        if os.path.exists(wheelchair_depth_path):
            wheelchair_depth_image = Image.open(wheelchair_depth_path).convert("L")
            wheelchair_depth_image = transforms.ToTensor()(wheelchair_depth_image)
        else:
            print(f"Warning: {wheelchair_depth_path} not found. Using a blank depth image.")
            wheelchair_depth_image = torch.zeros((1, 64, 64))

        return (gen3_rgb_image, gen3_depth_image, wheelchair_rgb_image, wheelchair_depth_image,
                instruction_embedding, action, time_info, trial_id, label)

import os
import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import torchvision.transforms as transforms
from PIL import Image

class RobotTaskSequenceDataset(Dataset):
    def __init__(
        self,
        csv_task_path,
        num_trials,
        word2vec_model,
        seq_len=50,
        step=1,
        rgb_size=(224, 224),
        depth_size=(64, 64)
    ):
        """
        Args:
            csv_task_path (str): Path to the directory containing CSV files (Experiment1, Experiment2, etc.).
            num_trials (int): Number of experiments/trials.
            word2vec_model: A pre-trained word2vec model or a dict {token -> vector}.
            seq_len (int): Number of consecutive time-steps per sequence.
            rgb_size (tuple): (H, W) for RGB resizing (if desired).
            depth_size (tuple): (H, W) for depth resizing (if desired).
        """
        self.csv_task_path = csv_task_path
        self.num_trials = num_trials
        self.word2vec = word2vec_model
        self.seq_len = seq_len
        self.step = step

        # Determine word embedding dimension
        try:
            self.embedding_dim = self.word2vec.vector_size
        except AttributeError:
            # If `word2vec` is a dict
            first_key = next(iter(self.word2vec))
            self.embedding_dim = len(self.word2vec[first_key])

        # We'll store each trial folder path so we can build image paths.
        self.wheelchair_rgb_paths = []
        self.wheelchair_depth_paths = []
        self.gen3_rgb_paths = []
        self.gen3_depth_paths = []

        data_frames = []
        for n in range(1, num_trials + 1):
            csv_path = os.path.join(self.csv_task_path, f'Experiment{n}', 'final_data.csv')
            self.wheelchair_rgb_paths.append(csv_path.replace('final_data.csv', 'wheelchair_rgb/'))
            self.wheelchair_depth_paths.append(csv_path.replace('final_data.csv', 'wheelchair_depth/'))
            self.gen3_rgb_paths.append(csv_path.replace('final_data.csv', 'gen3_rgb/'))
            self.gen3_depth_paths.append(csv_path.replace('final_data.csv', 'gen3_depth/'))

            df = pd.read_csv(csv_path)
            # Ensure we have a 'frame_number' column (or create one if needed)
            if 'image_frame_id' not in df.columns:
                # Example of numbering frames within each trial_id
                df['image_frame_id'] = df.groupby('trial_id').cumcount()
            data_frames.append(df)

        # Combine into one DataFrame
        self.data = pd.concat(data_frames, ignore_index=True)
        # Sort by trial_id, then frame_number
        self.data = self.data.sort_values(['trial_id', 'image_frame_id'])

        # Group by trial_id to form sequences
        grouped = self.data.groupby('trial_id')

        self.sequences = []
        last_start = len(frames) - seq_len + 1
        for trial_id, group_df in grouped:
            # Convert each row to a dict (so we can easily access columns later)
            frames = group_df.to_dict('records')
            last_start = len(frames) - seq_len + 1
            # Slide a window of length seq_len
            if last_start < 0:
                # If there's not enough frames for even one full subsequence, skip
                continue

            for start_idx in range(0, last_start, self.step):
                end_idx = start_idx + seq_len
                subseq_frames = frames[start_idx:end_idx]
                self.sequences.append({
                    'trial_id': trial_id,
                    'frames': subseq_frames
                })

        # Define image transforms
        # You can adjust these as needed (resize, normalization, etc.)
        self.rgb_transform = transforms.Compose([
            transforms.Resize(rgb_size),
            transforms.ToTensor()
        ])
        self.depth_transform = transforms.Compose([
            transforms.Resize(depth_size),
            transforms.ToTensor()
        ])

    def tokenize_instructions(self, instruction):
      tokens = instruction.split()
      vectors = []
      for token in tokens:
          if token in self.word2vec:
              vec = self.word2vec[token]
          else:
              print(f"Token not found in word2vec: {token}")  # Debug statement
              if '<unk>' in self.word2vec:
                  vec = self.word2vec['<unk>']
              else:
                  vec = [0.0] * self.embedding_dim
          vectors.append(vec)

      if len(vectors) == 0:
          avg_vec = [0.0] * self.embedding_dim
      else:
          avg_vec = [sum(x) / len(vectors) for x in zip(*vectors)]
      return torch.tensor(avg_vec, dtype=torch.float32)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        seq_info = self.sequences[idx]
        frames = seq_info['frames']

        # Lists to accumulate data over seq_len time-steps
        gen3_rgb_seq = []
        gen3_depth_seq = []
        wheelchair_rgb_seq = []
        wheelchair_depth_seq = []
        instruction_seq = []
        action_seq = []
        time_seq = []
        trial_id_seq = []
        label_seq = []

        for row in frames:
            trial_id_val = row['trial_id']
            frame_number = int(row['image_frame_id'])

            # Build image paths
            gen3_rgb_path = os.path.join(self.gen3_rgb_paths[trial_id_val], f"frame_{frame_number}.png")
            gen3_depth_path = os.path.join(self.gen3_depth_paths[trial_id_val], f"frame_{frame_number}.png")
            wc_rgb_path = os.path.join(self.wheelchair_rgb_paths[trial_id_val], f"frame_{frame_number}.png")
            wc_depth_path = os.path.join(self.wheelchair_depth_paths[trial_id_val], f"frame_{frame_number}.png")

            # Load images (or placeholders if missing)
            # --- Gen3 RGB ---
            if os.path.exists(gen3_rgb_path):
                gen3_rgb_img = Image.open(gen3_rgb_path).convert("RGB")
                gen3_rgb_img = self.rgb_transform(gen3_rgb_img)
            else:
                # Fallback to a blank image
                gen3_rgb_img = Image.new("RGB", (224, 224))
                gen3_rgb_img = self.rgb_transform(gen3_rgb_img)

            # --- Gen3 Depth ---
            if os.path.exists(gen3_depth_path):
                gen3_depth_img = Image.open(gen3_depth_path).convert("L")
                gen3_depth_img = self.depth_transform(gen3_depth_img)
            else:
                gen3_depth_img = Image.new("L", (64, 64))
                gen3_depth_img = self.depth_transform(gen3_depth_img)

            # --- Wheelchair RGB ---
            if os.path.exists(wc_rgb_path):
                wc_rgb_img = Image.open(wc_rgb_path).convert("RGB")
                wc_rgb_img = self.rgb_transform(wc_rgb_img)
            else:
                wc_rgb_img = Image.new("RGB", (224, 224))
                wc_rgb_img = self.rgb_transform(wc_rgb_img)

            # --- Wheelchair Depth ---
            if os.path.exists(wc_depth_path):
                wc_depth_img = Image.open(wc_depth_path).convert("L")
                wc_depth_img = self.depth_transform(wc_depth_img)
            else:
                wc_depth_img = Image.new("L", (64, 64))
                wc_depth_img = self.depth_transform(wc_depth_img)

            # Instruction embedding
            instr_embed = self.tokenize_instructions(row['instructions'])

            # Action vector (assuming the first 112 columns are action values)
            # Adjust if your CSV columns differ.
            # If row is a dict with named columns, you might need to adapt this logic.
            # For example, your CSV might have columns [action_0, action_1, ..., action_111].
            # This snippet uses row.iloc in the single-step code, but here row is a dict.
            # Youâ€™ll need the actual column names for the 112 actions or a suitable approach.
            # Here is a naive example, but you MUST adapt to your real column names:
            action_values = []
            for i in range(112):
                # Suppose the CSV columns for actions are named "act_0", "act_1", ... "act_111"
                # or you store them differently. This is just an example:
                col_name = f"act_{i}"
                action_values.append(row[col_name])
            action_tensor = torch.tensor(action_values, dtype=torch.float32)

            # Time
            time_val = float(row['sim_time'])
            time_tensor = torch.tensor(time_val, dtype=torch.float32)

            # Label
            label_val = int(row['state_labels'])
            label_tensor = torch.tensor(label_val, dtype=torch.long)

            # Accumulate
            gen3_rgb_seq.append(gen3_rgb_img)
            gen3_depth_seq.append(gen3_depth_img)
            wheelchair_rgb_seq.append(wc_rgb_img)
            wheelchair_depth_seq.append(wc_depth_img)
            instruction_seq.append(instr_embed)
            action_seq.append(action_tensor)
            time_seq.append(time_tensor)
            trial_id_seq.append(torch.tensor(trial_id_val, dtype=torch.long))
            label_seq.append(label_tensor)

        # Now stack them into shape (seq_len, ...)
        gen3_rgb_seq = torch.stack(gen3_rgb_seq, dim=0)         # (seq_len, 3, H, W)
        gen3_depth_seq = torch.stack(gen3_depth_seq, dim=0)     # (seq_len, 1, H, W)
        wheelchair_rgb_seq = torch.stack(wheelchair_rgb_seq, dim=0)
        wheelchair_depth_seq = torch.stack(wheelchair_depth_seq, dim=0)
        instruction_seq = torch.stack(instruction_seq, dim=0)   # (seq_len, embedding_dim)
        action_seq = torch.stack(action_seq, dim=0)             # (seq_len, 112)
        time_seq = torch.stack(time_seq, dim=0)                 # (seq_len,)
        trial_id_seq = torch.stack(trial_id_seq, dim=0)         # (seq_len,)
        label_seq = torch.stack(label_seq, dim=0)               # (seq_len,)

        return (gen3_rgb_seq, gen3_depth_seq,
                wheelchair_rgb_seq, wheelchair_depth_seq,
                instruction_seq, action_seq,
                time_seq, trial_id_seq, label_seq)


# ------------------
# Example usage
# ------------------
# if __name__ == "__main__":
#     # Suppose you have a word2vec model or dictionary:
#     dummy_w2v = {"hello": [1.0, 2.0], "world": [3.0, 4.0], "<unk>": [0.0, 0.0]}

#     # Create dataset
#     dataset = RobotTaskSequenceDataset(
#         csv_task_path="/path/to/your/data",
#         num_trials=3,
#         word2vec_model=dummy_w2v,
#         seq_len=10
#     )

#     # Each __getitem__ returns an entire sequence of length seq_len
#     # If you set batch_size=2, you'll get 2 sequences per batch
#     dataloader = DataLoader(dataset, batch_size=2, shuffle=False)

#     for batch in dataloader:
#         (gen3_rgb_seq, gen3_depth_seq,
#          wheelchair_rgb_seq, wheelchair_depth_seq,
#          instruction_seq, action_seq,
#          time_seq, trial_id_seq, label_seq) = batch

#         # Shapes:
#         #   gen3_rgb_seq: (batch_size, seq_len, 3, H, W)
#         #   instruction_seq: (batch_size, seq_len, embedding_dim)
#         # etc.

#         print("gen3_rgb_seq shape:", gen3_rgb_seq.shape)
#         print("label_seq shape:", label_seq.shape)
#         break  # Just one batch for demo

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import gensim.downloader as api

# Example device selection
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters
num_labels = 4         # Number of classification labels
pred_dim = 112         # Dimensionality of the next action prediction output
num_trials = 7         # Total number of different trials (for trial embedding)
lstm_hidden_dim = 128
lstm_layers = 1
learning_rate = 1e-4
weight_decay = 1e-4
num_epochs = 10
seq_len = 10
step = 1
rgb_size = (224, 224)
depth_size = (64, 64)
csv_task_path = '/content/drive/MyDrive/data/organization/picking/pick_mustard'
batch_size = 2
number_workers = 2

# ---------------------------------------------------------------------------
# Load pre-trained word2vec
# (Replace this with your actual loading if needed)
# ---------------------------------------------------------------------------
word2vec_model = api.load("word2vec-google-news-300")
# ---------------------------------------------------------------------------
# Model definition (assume MultiModalityModel matches the data shapes)
# ---------------------------------------------------------------------------
model = MultiModalityModel(
    num_labels=num_labels,
    pred_dim=pred_dim,
    num_trials=num_trials,
    word2vec_matrix=word2vec_model,
    lstm_hidden_dim=lstm_hidden_dim,
    lstm_layers=lstm_layers
)
model.to(device)

# ---------------------------------------------------------------------------
# Optimizer, Loss
# ---------------------------------------------------------------------------
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
criterion_cls = nn.CrossEntropyLoss()  # For classification

# ---------------------------------------------------------------------------
# Create your sequence dataset + DataLoader
# ---------------------------------------------------------------------------
train_dataset = RobotTaskSequenceDataset(
    csv_task_path=csv_task_path,
    num_trials=num_trials,
    word2vec_model=word2vec_model,
    seq_len=seq_len,       # each item = sub-sequence of length seq_len
    step=step,
    rgb_size=rgb_size,
    depth_size=depth_size
)

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=number_workers
)

# ---------------------------------------------------------------------------
# Training Loop
# ---------------------------------------------------------------------------
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for batch_idx, batch in enumerate(train_loader):
        # The dataset returns 9 items, each with shape (B, seq_len, ...)
        (
            gen3_rgb_seq, gen3_depth_seq,
            wheelchair_rgb_seq, wheelchair_depth_seq,
            instruction_seq, action_seq,
            time_seq, trial_id_seq,
            label_seq
        ) = batch

        # Move data to device
        gen3_rgb_seq = gen3_rgb_seq.to(device)            # (B, seq_len, 3, H, W)
        gen3_depth_seq = gen3_depth_seq.to(device)        # (B, seq_len, 1, H, W)
        wheelchair_rgb_seq = wheelchair_rgb_seq.to(device)# (B, seq_len, 3, H, W)
        wheelchair_depth_seq = wheelchair_depth_seq.to(device)
        instruction_seq = instruction_seq.to(device)      # (B, seq_len, embedding_dim)
        action_seq = action_seq.to(device)                # (B, seq_len, 112)
        time_seq = time_seq.to(device)                    # (B, seq_len)
        trial_id_seq = trial_id_seq.to(device)            # (B, seq_len)
        label_seq = label_seq.to(device)                  # (B, seq_len) each time-step might have a label

        # If your model outputs one classification (B, num_labels),
        # pick a single label from the sequence. e.g., last time-step:
        label_for_loss = label_seq[:, -1]                 # (B,)

        # Reset gradients
        optimizer.zero_grad()

        # Forward pass
        # model(...) should expect these shapes as described above
        cls_logits, next_action_pred = model(
            image1_seq=gen3_rgb_seq,
            image2_seq=wheelchair_rgb_seq,
            instruction_seq=instruction_seq,
            action_seq=action_seq,
            depth1_seq=gen3_depth_seq,
            depth2_seq=wheelchair_depth_seq,
            time_seq=time_seq,
            trial_id_seq=trial_id_seq
        )

        # cls_logits shape: (B, num_labels)
        # next_action_pred shape: (B, pred_dim)

        # Classification loss using label_for_loss
        loss_cls = criterion_cls(cls_logits, label_for_loss)

        # If you also want next-action loss, define it here:
        # next_action_loss = ...
        # total_loss = loss_cls + 0.1 * next_action_loss
        # For now, just classification:
        loss = loss_cls

        # Backprop
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")

# ---------------------------------------------------------------------------
# (Optional) Save the model
# ---------------------------------------------------------------------------
torch.save(model.state_dict(), "multimodality_model_sequence.pth")
