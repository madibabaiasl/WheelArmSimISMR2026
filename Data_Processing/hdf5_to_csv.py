# -*- coding: utf-8 -*-
"""hdf5_to_csv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qNV9rZzIs2QchBilT40K5FhTxEYW0M7H

### **This script is used to convert hdf5 filt to a .csv file**
"""

from google.colab import drive
drive.mount('/content/drive')

n = 1

"""### This is to process a single file"""

import h5py
import csv
import re

# Path to the HDF5 file
file_path = f'/content/drive/MyDrive/data/organization/picking/pick_mustard/Experiment{n}/experiment{n}.hdf5'
output_csv = f'/content/drive/MyDrive/data/organization/picking/pick_mustard/Experiment{n}/experiment{n}.csv'

# List of dataset names to skip (match suffix, regardless of parent group)
skip_suffixes = [
    'processed_data_cameras_depth_gen3',
    'processed_data_cameras_depth_wheelchair',
    'processed_data_cameras_rgb_gen3',
    'processed_data_cameras_rgb_wheelchair'
]

# Dictionary to store all groups and their datasets
data_dict = {}

# Open the HDF5 file and explore structure
with h5py.File(file_path, 'r') as hdf:

    def explore_hdf5(group, prefix=''):
        """ Recursively explore HDF5 groups and datasets """
        group_name = prefix if prefix else "/"
        group_data = {"Group Name": group_name}  # First column is group name
        # print(f"group name {group_name}")

        for key in group.keys():
            item = group[key]
            item_path = f"{prefix}/{key}".strip("/")  # Full dataset path

            if isinstance(item, h5py.Group):
                # Recursively explore subgroups
                explore_hdf5(item, item_path)

            elif isinstance(item, h5py.Dataset):
                # Skip unwanted datasets
                if any(item_path.endswith(suffix) for suffix in skip_suffixes):
                    continue

                try:
                    # Detect scalar or array dataset
                    if item.shape == ():
                        data_content = item[()]  # Scalar datasets
                    else:
                        data_content = item[:]  # Array datasets

                    # Convert arrays to lists, handle scalar values
                    group_data[key] = data_content.tolist() if hasattr(data_content, "tolist") else data_content
                except Exception as e:
                    print(f"Error reading dataset {item_path}: {e}")
                    group_data[key] = "Error"

        # Store group data
        data_dict[group_name] = group_data

    # Start exploring the structure from root
    explore_hdf5(hdf)

def extract_number(group_name):
    """Extracts the numeric part from '/data/N' for correct sorting."""
    match = re.search(r'data/(\d+)$', group_name)  # Extracts only the number at the end
    return int(match.group(1)) if match else float('inf')  # Convert to int, or inf if no match


# sorted_groups = sorted(data_dict.keys(), key=extract_number)  # Sort numerically
sorted_groups = sorted(data_dict.keys(), key=extract_number)  # Now sorts numerically!


# Generate column headers dynamically
all_columns = set()
for group_data in data_dict.values():
    all_columns.update(group_data.keys())

all_columns = sorted(all_columns)  # Sort for consistent column order

# Write data to CSV
with open(output_csv, mode='w', newline='') as csv_file:
    csv_writer = csv.writer(csv_file)

    # Write header
    csv_writer.writerow(all_columns)

    # Write rows, maintaining sorted order
    for group_name in sorted_groups:
        # print(f"group_name {group_name}")
        group_data = data_dict[group_name]
        csv_writer.writerow([group_data.get(col, "") for col in all_columns])

"""### This is to process experiments in one task"""

import h5py
import csv
import re
import os

SKIP_SUFFIXES = [
    'processed_data_cameras_depth_gen3',
    'processed_data_cameras_depth_wheelchair',
    'processed_data_cameras_rgb_gen3',
    'processed_data_cameras_rgb_wheelchair'
]

def extract_number(group_name):
    """Extract the numeric part from '/data/N' for correct sorting."""
    match = re.search(r'data/(\d+)$', group_name)  # Extracts only the number at the end
    return int(match.group(1)) if match else float('inf')  # Convert to int, or inf if no match

def hdf5_to_csv(input_hdf5, output_csv):
    """
    Converts a single HDF5 file to a CSV file.
    Skips datasets whose names end with entries in SKIP_SUFFIXES.
    """
    data_dict = {}

    def explore_hdf5(group, prefix=''):
        """Recursively explore HDF5 groups and datasets."""
        group_name = prefix if prefix else "/"
        group_data = {"Group Name": group_name}  # Will appear as a column

        for key in group.keys():
            item = group[key]
            item_path = f"{prefix}/{key}".strip("/")  # Full dataset path

            if isinstance(item, h5py.Group):
                # Recursively explore subgroups
                explore_hdf5(item, item_path)
            elif isinstance(item, h5py.Dataset):
                # Skip unwanted datasets based on suffix
                if any(item_path.endswith(suffix) for suffix in SKIP_SUFFIXES):
                    continue
                try:
                    # Detect scalar or array dataset
                    if item.shape == ():
                        data_content = item[()]  # Scalar
                    else:
                        data_content = item[:]   # Array

                    # Convert arrays to lists
                    group_data[key] = (
                        data_content.tolist()
                        if hasattr(data_content, "tolist")
                        else data_content
                    )
                except Exception as e:
                    print(f"Error reading dataset {item_path}: {e}")
                    group_data[key] = "Error"

        # Store the group's data
        data_dict[group_name] = group_data

    # --- End of explore_hdf5() ---

    # Open the HDF5 file and explore
    with h5py.File(input_hdf5, 'r') as hdf:
        explore_hdf5(hdf)

    # Sort group keys by numeric part in '/data/N'
    sorted_groups = sorted(data_dict.keys(), key=extract_number)

    # Build a list of all columns we might need
    all_columns = set()
    for group_data in data_dict.values():
        all_columns.update(group_data.keys())

    all_columns = sorted(all_columns)  # Sort columns for consistency

    # Write out the CSV
    with open(output_csv, mode='w', newline='') as csv_file:
        csv_writer = csv.writer(csv_file)

        # Write header row
        csv_writer.writerow(all_columns)

        # Write data rows
        for group_name in sorted_groups:
            group_data = data_dict[group_name]
            row_data = [group_data.get(col, "") for col in all_columns]
            csv_writer.writerow(row_data)

import glob

def main():
    # Base directory that contains Experiment1, Experiment2, etc.
    base_dir = "/content/drive/MyDrive/data/organization/picking/pick_mustard"

    # This pattern will match any subfolder starting with 'Experiment'
    experiment_dirs = glob.glob(os.path.join(base_dir, "Experiment*"))

    for exp_dir in experiment_dirs:
        # Make sure it's actually a directory
        if os.path.isdir(exp_dir):
            print(f"\n=== Processing directory: {exp_dir} ===")

            # Find all .hdf5 files in that directory
            hdf5_files = glob.glob(os.path.join(exp_dir, "*.hdf5"))

            for hdf5_file in hdf5_files:
                # Option 1: Just change extension from .hdf5 to .csv
                # output_csv = os.path.splitext(hdf5_file)[0] + ".csv"

                # Option 2: Append '_converted' to avoid any confusion
                # with an existing CSV of the same name
                base_name = os.path.splitext(hdf5_file)[0]
                output_csv = base_name + ".csv"

                print(f"  Converting {hdf5_file} -> {output_csv}")
                hdf5_to_csv(hdf5_file, output_csv)

if __name__ == "__main__":
    main()