# -*- coding: utf-8 -*-
"""save_images_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13azhPEkzKY5pj6jzB3N4DdjVlFrqfvaq
"""

# -*- coding: utf-8 -*-
"""
save_images_all_experiments.ipynb

This script is used to save images from experiment1.hdf5 through experiment20.hdf5
into separate folders within your Google Drive.

It handles four different datasets in each experiment:
  1. wheelchair_rgb
  2. gen3_rgb
  3. gen3_depth
  4. wheelchair_depth
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import re
import h5py
import numpy as np
import matplotlib.pyplot as plt

# All experiment indices you want to process:
experiment_range = range(1, 21)

# -----------------------------------------------------------------------------
# Section 1: Wheelchair RGB
# -----------------------------------------------------------------------------

height, width, channels = 720, 1280, 3
frame_size = height * width * channels  # 2,764,800 for 720×1280×3

# Regex to match dataset paths like "data/<integer>/processed_data_cameras_rgb_wheelchair"
dataset_pattern_wheel_rgb = re.compile(r'^data/(\d+)/processed_data_cameras_rgb_wheelchair$')

def explore_and_extract_images_wheel_rgb(group, prefix, save_dir):
    """
    Recursively explore the HDF5 'group'.
    If we find a dataset matching /data/<N>/processed_data_cameras_rgb_wheelchair,
    we save it as frame_<N>.png.
    """
    for key in group.keys():
        item = group[key]

        # Construct the dataset path (no leading slash)
        item_path = f"{prefix}/{key}" if prefix else key

        if isinstance(item, h5py.Group):
            # Recurse into subgroups
            explore_and_extract_images_wheel_rgb(item, item_path, save_dir)
        elif isinstance(item, h5py.Dataset):
            match = dataset_pattern_wheel_rgb.match(item_path)
            if match:
                dataset_number_str = match.group(1)
                try:
                    dataset_number = int(dataset_number_str)
                except ValueError:
                    continue

                print(f"\nMatched wheelchair_rgb: /{item_path} -> dataset number: {dataset_number}")

                data = item[:]
                total_pixels = data.size
                print(f"  -> shape: {data.shape} (expected {frame_size})")

                # Check if there's exactly one image worth of data
                if data.ndim == 1 and total_pixels == frame_size:
                    frame = data.reshape((height, width, channels)).astype(np.uint8)

                    out_filename = f"frame_{dataset_number}.png"
                    out_path = os.path.join(save_dir, out_filename)
                    plt.imsave(out_path, frame)
                    print(f"    -> Saved: {out_path}")
                else:
                    print("    -> Skipped (not exactly one frame)")

# -----------------------------------------------------------------------------
# Section 2: Gen3 RGB
# -----------------------------------------------------------------------------
dataset_pattern_gen3_rgb = re.compile(r'^data/(\d+)/processed_data_cameras_rgb_gen3$')

def explore_and_extract_images_gen3_rgb(group, prefix, save_dir):
    """
    Recursively explore the HDF5 'group'.
    If we find a dataset matching /data/<N>/processed_data_cameras_rgb_gen3,
    we save it as frame_<N>.png.
    """
    for key in group.keys():
        item = group[key]

        item_path = f"{prefix}/{key}" if prefix else key

        if isinstance(item, h5py.Group):
            explore_and_extract_images_gen3_rgb(item, item_path, save_dir)
        elif isinstance(item, h5py.Dataset):
            match = dataset_pattern_gen3_rgb.match(item_path)
            if match:
                dataset_number_str = match.group(1)
                try:
                    dataset_number = int(dataset_number_str)
                except ValueError:
                    continue

                print(f"\nMatched gen3_rgb: /{item_path} -> dataset number: {dataset_number}")

                data = item[:]
                total_pixels = data.size
                print(f"  -> shape: {data.shape} (expected {frame_size})")

                if data.ndim == 1 and total_pixels == frame_size:
                    frame = data.reshape((height, width, channels)).astype(np.uint8)

                    out_filename = f"frame_{dataset_number}.png"
                    out_path = os.path.join(save_dir, out_filename)
                    plt.imsave(out_path, frame)
                    print(f"    -> Saved: {out_path}")
                else:
                    print("    -> Skipped (not exactly one frame)")

# -----------------------------------------------------------------------------
# Section 3: Gen3 Depth
# -----------------------------------------------------------------------------
height_d, width_d = 720, 1280
frame_size_depth = height_d * width_d

dataset_pattern_gen3_depth = re.compile(r'^data/(\d+)/processed_data_cameras_depth_gen3$')

def explore_and_extract_images_gen3_depth(group, prefix, save_dir):
    """
    Recursively explore the HDF5 'group'.
    If we find a dataset matching /data/<N>/processed_data_cameras_depth_gen3,
    we save it as frame_<N>.png (converted to float and visualized in gray).
    """
    for key in group.keys():
        item = group[key]

        item_path = f"{prefix}/{key}" if prefix else key

        if isinstance(item, h5py.Group):
            explore_and_extract_images_gen3_depth(item, item_path, save_dir)
        elif isinstance(item, h5py.Dataset):
            match = dataset_pattern_gen3_depth.match(item_path)
            if match:
                dataset_number_str = match.group(1)
                try:
                    dataset_number = int(dataset_number_str)
                except ValueError:
                    continue

                print(f"\nMatched gen3_depth: /{item_path} -> dataset number: {dataset_number}")

                data = item[:]
                total_pixels = data.size
                print(f"  -> shape: {data.shape} (expected {frame_size_depth})")

                # Convert depth data from flattened array of bytes into float32
                # Then reshape to 720x1280
                depth_image_uint8 = data.astype(np.uint8)
                depth_image_float32 = depth_image_uint8.view(np.float32)
                depth_image = depth_image_float32.reshape(height_d, width_d)

                out_filename = f"frame_{dataset_number}.png"
                out_path = os.path.join(save_dir, out_filename)
                plt.imsave(out_path, depth_image, cmap='gray')
                print(f"    -> Saved: {out_path}")

# -----------------------------------------------------------------------------
# Section 4: Wheelchair Depth
# -----------------------------------------------------------------------------
dataset_pattern_wheel_depth = re.compile(r'^data/(\d+)/processed_data_cameras_depth_wheelchair$')

def explore_and_extract_images_wheel_depth(group, prefix, save_dir):
    """
    Recursively explore the HDF5 'group'.
    If we find a dataset matching /data/<N>/processed_data_cameras_depth_wheelchair,
    we save it as frame_<N>.png (converted to float and visualized in gray).
    """
    for key in group.keys():
        item = group[key]

        item_path = f"{prefix}/{key}" if prefix else key

        if isinstance(item, h5py.Group):
            explore_and_extract_images_wheel_depth(item, item_path, save_dir)
        elif isinstance(item, h5py.Dataset):
            match = dataset_pattern_wheel_depth.match(item_path)
            if match:
                dataset_number_str = match.group(1)
                try:
                    dataset_number = int(dataset_number_str)
                except ValueError:
                    continue

                print(f"\nMatched wheelchair_depth: /{item_path} -> dataset number: {dataset_number}")

                data = item[:]
                total_pixels = data.size
                print(f"  -> shape: {data.shape} (expected {frame_size_depth})")

                depth_image_uint8 = data.astype(np.uint8)
                depth_image_float32 = depth_image_uint8.view(np.float32)
                depth_image = depth_image_float32.reshape(height_d, width_d)

                out_filename = f"frame_{dataset_number}.png"
                out_path = os.path.join(save_dir, out_filename)
                plt.imsave(out_path, depth_image, cmap='gray')
                print(f"    -> Saved: {out_path}")


# -----------------------------------------------------------------------------
# Main Loop: Process each experiment n = 1..20
# -----------------------------------------------------------------------------

for n in experiment_range:
    print("\n========================")
    print(f"PROCESSING EXPERIMENT {n}")
    print("========================")

    # Path to the HDF5 file for this experiment
    file_path = f'/content/drive/MyDrive/data/organization/picking/pick_mustard/Experiment{n}/experiment{n}.hdf5'

    # 1) Wheelchair RGB
    wheelchair_rgb_dir = f'/content/drive/MyDrive/data/organization/picking/pick_mustard/Experiment{n}/wheelchair_rgb'
    os.makedirs(wheelchair_rgb_dir, exist_ok=True)

    # 2) Gen3 RGB
    gen3_rgb_dir = f'/content/drive/MyDrive/data/organization/picking/pick_mustard/Experiment{n}/gen3_rgb'
    os.makedirs(gen3_rgb_dir, exist_ok=True)

    # 3) Gen3 Depth
    gen3_depth_dir = f'/content/drive/MyDrive/data/organization/picking/pick_mustard/Experiment{n}/gen3_depth'
    os.makedirs(gen3_depth_dir, exist_ok=True)

    # 4) Wheelchair Depth
    wheelchair_depth_dir = f'/content/drive/MyDrive/data/organization/picking/pick_mustard/Experiment{n}/wheelchair_depth'
    os.makedirs(wheelchair_depth_dir, exist_ok=True)

    # Open the HDF5 file once and extract all four data types
    with h5py.File(file_path, "r") as hdf:
        # Wheelchair RGB
        explore_and_extract_images_wheel_rgb(hdf, prefix="", save_dir=wheelchair_rgb_dir)
        # Gen3 RGB
        explore_and_extract_images_gen3_rgb(hdf, prefix="", save_dir=gen3_rgb_dir)
        # Gen3 Depth
        explore_and_extract_images_gen3_depth(hdf, prefix="", save_dir=gen3_depth_dir)
        # Wheelchair Depth
        explore_and_extract_images_wheel_depth(hdf, prefix="", save_dir=wheelchair_depth_dir)

print("All experiments processed!")
